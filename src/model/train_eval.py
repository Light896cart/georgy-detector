from typing import Optional, Dict, Any

import torch.optim as optim
import torch
import torch.nn as nn
from matplotlib import pyplot as plt
from torch.optim.lr_scheduler import CosineAnnealingLR
from tqdm import tqdm
from torch.utils.data import DataLoader


def train_and_evaluate_model(
    model: nn.Module,
    train_loader: DataLoader,
    val_loader: DataLoader,
    epochs: int = 10,
    lr: float = 1e-3,
    device: str = None,
    load_weights_path: Optional[str] = "best_model.pth",  # –æ—Ç–∫—É–¥–∞ –∑–∞–≥—Ä—É–∂–∞—Ç—å (–º–æ–∂–µ—Ç –±—ã—Ç—å None)
    save_best_path: str = "best_model_2.pth",  # –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –ª—É—á—à—É—é
    verbose: bool = True,
    seed: int = 42,
    weight_decay: float = 1e-4,
    use_scheduler: bool = True,
    patience: int = 5,
) -> Dict[str, Any]:
    """
    –û–±—É—á–∞–µ—Ç –º–æ–¥–µ–ª—å —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π. –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –∑–∞–≥—Ä—É–∑–∫—É –≤–µ—Å–æ–≤ –∏–∑ –æ–¥–Ω–æ–≥–æ —Ñ–∞–π–ª–∞
    –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–µ–π –º–æ–¥–µ–ª–∏ –≤ –¥—Ä—É–≥–æ–π.

    Args:
        model: PyTorch –º–æ–¥–µ–ª—å
        train_loader: DataLoader –¥–ª—è –æ–±—É—á–µ–Ω–∏—è
        val_loader: DataLoader –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏
        epochs: —á–∏—Å–ª–æ —ç–ø–æ—Ö
        lr: learning rate
        device: 'cuda' –∏–ª–∏ 'cpu', –µ—Å–ª–∏ None ‚Äî –∞–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–∏
        load_weights_path: –ø—É—Ç—å –∫ –≤–µ—Å–∞–º –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ (None = –Ω–∞—á–∞—Ç—å —Å –Ω—É–ª—è)
        save_best_path: –∫—É–¥–∞ —Å–æ—Ö—Ä–∞–Ω–∏—Ç—å –ª—É—á—à—É—é –º–æ–¥–µ–ª—å
        verbose: –≤—ã–≤–æ–¥–∏—Ç—å –ª–∏ –ø—Ä–æ–≥—Ä–µ—Å—Å
        seed: seed –¥–ª—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç–∏
        weight_decay: L2-—Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
        use_scheduler: –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ª–∏ cosine annealing
        patience: –æ—Å—Ç–∞–Ω–æ–≤–∫–∞, –µ—Å–ª–∏ val_acc –Ω–µ —Ä–∞—Å—Ç—ë—Ç N —ç–ø–æ—Ö

    Returns:
        dict —Å –º–µ—Ç—Ä–∏–∫–∞–º–∏ –∏ –º–æ–¥–µ–ª—å—é
    """
    # --- üå± –í–æ—Å–ø—Ä–æ–∏–∑–≤–æ–¥–∏–º–æ—Å—Ç—å ---
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed_all(seed)

    # --- üñ•Ô∏è Device setup ---
    if device is None:
        device = "cuda" if torch.cuda.is_available() else "cpu"
    model.to(device)

    # --- üîÅ –ó–∞–≥—Ä—É–∑–∫–∞ –≤–µ—Å–æ–≤ (–µ—Å–ª–∏ —É–∫–∞–∑–∞–Ω –ø—É—Ç—å) ---
    if load_weights_path is not None:
        try:
            checkpoint = torch.load(load_weights_path, map_location=device)
            if 'state_dict' in checkpoint:
                model.load_state_dict(checkpoint['state_dict'])
            else:
                model.load_state_dict(checkpoint)
            if verbose:
                print(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω—ã –≤–µ—Å–∞ –∏–∑: {load_weights_path}")
        except FileNotFoundError:
            if verbose:
                print(f"‚ö†Ô∏è –í–µ—Å–∞ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã: {load_weights_path}. –ù–∞—á–∏–Ω–∞–µ–º —Å –Ω—É–ª—è.")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –ø—Ä–∏ –∑–∞–≥—Ä—É–∑–∫–µ –≤–µ—Å–æ–≤ –∏–∑ {load_weights_path}: {e}")
    else:
        if verbose:
            print("üöÄ –ù–∞—á–∏–Ω–∞–µ–º –æ–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è (load_weights_path=None).")

    # --- ‚öôÔ∏è –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä –∏ –ª–æ—Å—Å ---
    optimizer = optim.AdamW(model.parameters(), lr=lr, weight_decay=weight_decay)
    criterion = nn.CrossEntropyLoss()
    scheduler = CosineAnnealingLR(optimizer, T_max=epochs) if use_scheduler else None

    # --- üìà –õ–æ–≥–∏ ---
    train_losses = []
    val_accuracies = []
    best_val_acc = 0.0
    epochs_no_improve = 0

    # --- üîÅ –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è ---
    for epoch in range(epochs):
        # --- üü† –û–±—É—á–µ–Ω–∏–µ ---
        model.train()
        running_loss = 0.0
        progress_bar = tqdm(
            train_loader,
            desc=f"Epoch {epoch + 1}/{epochs}",
            unit="batch",
            disable=not verbose,
            leave=False
        )

        for images, labels in progress_bar:
            images = images.to(device, non_blocking=True)
            labels = labels.to(device, non_blocking=True)

            optimizer.zero_grad()
            outputs = model(images)
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_loss += loss.item() * images.size(0)
            progress_bar.set_postfix(loss=f"{loss.item():.4f}")

        epoch_loss = running_loss / len(train_loader)
        train_losses.append(epoch_loss)

        # --- üü¢ –í–∞–ª–∏–¥–∞—Ü–∏—è ---
        model.eval()
        correct = 0
        total = 0
        with torch.no_grad():
            for images, labels in val_loader:
                images = images.to(device, non_blocking=True)
                labels = labels.to(device, non_blocking=True)
                outputs = model(images)
                _, predicted = torch.max(outputs, 1)
                total += labels.size(0)
                correct += (predicted == labels).sum().item()

        val_acc = correct / total
        val_accuracies.append(val_acc)

        # --- üèÜ –°–æ—Ö—Ä–∞–Ω—è–µ–º –õ–£–ß–®–£–Æ –º–æ–¥–µ–ª—å –≤ save_best_path ---
        if val_acc > best_val_acc:
            best_val_acc = val_acc
            torch.save({
                'state_dict': model.state_dict(),
                'epoch': epoch,
                'val_acc': val_acc,
                'train_loss': epoch_loss,
                'model_config': {
                    'architecture': model.__class__.__name__,
                    'num_classes': model.fc.out_features,
                }
            }, save_best_path)
            if verbose:
                print(f"üéâ –ù–æ–≤–∞—è –ª—É—á—à–∞—è –º–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞ –≤: {save_best_path} (acc={val_acc:.4f})")
            epochs_no_improve = 0
        else:
            epochs_no_improve += 1

        # --- üì¢ –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ ---
        if verbose:
            print(f"Epoch [{epoch+1:2d}/{epochs}] | Loss: {epoch_loss:6.4f} | Val Acc: {val_acc:6.4f} | Best: {best_val_acc:6.4f}")

        # --- üõë Early stopping ---
        if patience is not None and epochs_no_improve >= patience:
            if verbose:
                print(f"üõë –†–∞–Ω–Ω—è—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ –Ω–∞ —ç–ø–æ—Ö–µ {epoch+1}")
            break

        # --- üîÑ LR Scheduler ---
        if scheduler is not None:
            scheduler.step()

    # --- üìà –ì—Ä–∞—Ñ–∏–∫–∏ ---
    plt.figure(figsize=(12, 5))
    plt.subplot(1, 2, 1)
    plt.plot(train_losses, label='Training Loss', color='tab:orange', linewidth=2)
    plt.title('Training Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.grid(True, alpha=0.3)
    plt.legend()

    plt.subplot(1, 2, 2)
    plt.plot(val_accuracies, label='Validation Accuracy', color='tab:green', linewidth=2)
    plt.axhline(y=best_val_acc, color='r', linestyle='--', label=f'Best: {best_val_acc:.4f}')
    plt.title('Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.grid(True, alpha=0.3)
    plt.legend()
    plt.tight_layout()
    plt.show()

    return {
        "best_val_acc": best_val_acc,
        "train_losses": train_losses,
        "val_accuracies": val_accuracies,
        "model": model,
        "device": device,
    }